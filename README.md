# Research Copilot

**Research Copilot** is an **agentic literature analysis system** that automates the journey from raw research papers to structured insights, synthesis, and critique using Large Language Models (LLMs).

It is built to help researchers, students, and engineers **rapidly understand a research area**, identify trends, gaps, and evaluate the quality of synthesized conclusions.

---

##  What This Project Does

Given a research topic, Research Copilot:

1.  Searches arXiv for relevant papers  
2.  Downloads and parses PDFs  
3.  Extracts structured metadata using an LLM agent  
4.  Synthesizes insights across papers  
5.  Critiques the synthesis using a Critic Agent  
6.  Visualizes everything in a Streamlit UI  

All stages are **modular, inspectable, and agent-driven**.

---

##  Architecture Overview

Topic
â”‚
â–¼
[ arXiv Ingestion ]
â”‚
â–¼
[ PDF Parsing ]
â”‚
â–¼
[ Extraction Agent ]
â”‚   â†’ task, method, datasets, metrics, results, limitations
â–¼
[ Synthesis Agent ]
â”‚   â†’ trends, dominant tasks, gaps, open questions
â–¼
[ Critic Agent ]
â†’ strengths, weaknesses, rating, suggested repairs

Each agent runs independently and writes outputs to **disk** and **SQLite**.

---

## Agents

### ğŸ” Extraction Agent
Reads parsed paper sections and produces structured JSON:
- task
- method
- datasets
- metrics
- key results
- limitations

### Synthesis Agent
Aggregates extractions to identify:
- dominant research directions
- common metrics and datasets
- gaps and open problems

### Critic Agent
Evaluates synthesis quality and outputs:
- overall rating (out of 10)
- strengths & weaknesses
- suggested repairs
- improved synthesis draft

---

## Streamlit UI

The Streamlit app provides:

-  One-click pipeline execution
-  Browse extracted papers
-  View synthesis outputs
-  Inspect critic feedback
-  Live pipeline logs

Run it with:

python -m streamlit run app/ui/streamlit_app.py


â¸»

âš™ï¸ Tech Stack

Core
	â€¢	Python 3.10+
	â€¢	SQLite
	â€¢	Streamlit

LLMs
	â€¢	Ollama (local models like mistral)
	â€¢	Gemini (optional, higher-quality synthesis & critique)

Parsing & Ingestion
	â€¢	PyMuPDF (PDF parsing)
	â€¢	feedparser (arXiv API)

â¸»

ğŸ“ Project Structure

research-copilot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”œâ”€â”€ extraction_agent.py
â”‚   â”‚   â”œâ”€â”€ synthesis_agent.py
â”‚   â”‚   â””â”€â”€ critic_agent.py
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ run_extraction.py
â”‚   â”‚   â”œâ”€â”€ run_synthesis.py
â”‚   â”‚   â”œâ”€â”€ run_critic.py
â”‚   â”‚   â””â”€â”€ run_full_pipeline.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ parsing/
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ streamlit_app.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pdfs/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ extracted/
â”‚   â”œâ”€â”€ synthesis/
â”‚   â””â”€â”€ critic/
â”œâ”€â”€ research.db
â”œâ”€â”€ .env.example
â””â”€â”€ README.md


â¸»

ğŸ” Configuration

Create one locally:

Example:

LLM_PROVIDER=ollama
OLLAMA_MODEL=mistral
OLLAMA_URL=http://localhost:11434

GEMINI_API_KEY=your_key_here
CRITIC_PROVIDER=gemini

DB_PATH=research.db


â¸»

ğŸš§ Known Limitations (v1)
	â€¢	arXiv API rate limiting
	â€¢	Occasional LLM JSON formatting errors
	â€¢	No embedding / RAG layer yet

These are design-acknowledged, not architectural blockers.

â¸»

ğŸš€ Future Work
	â€¢	Parallel RAG pipeline
	â€¢	Embeddings over extracted sections
	â€¢	Topic clustering & comparison
	â€¢	Report export (Markdown / LaTeX)
	â€¢	Cloud deployment



ğŸ‘¤ Author

Deepanshu Dawande
MSc Data Science & AI
Agentic Systems Â· LLM Evaluation Â· Applied ML
